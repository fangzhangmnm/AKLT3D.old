{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from opt_einsum import contract\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "def _toN(t):\n",
    "    return t.detach().cpu().numpy()\n",
    "def printDiff(name,value,ref):\n",
    "    if ref is not None:\n",
    "        print(name+':',value,'diff(abs):',value-ref)\n",
    "    else:\n",
    "        print(name+':',value)\n",
    "    \n",
    "import ast\n",
    "def eval_np_array_literal(array_string):\n",
    "    array_string = ','.join(array_string.replace('[ ', '[').split())\n",
    "    return np.array(ast.literal_eval(array_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRG BaseClass, can be used for HOTRG-like and XTRG-like contraction\n",
    "class SRG(torch.nn.Module):\n",
    "    def __init__(self,params,options):\n",
    "        super(SRG,self).__init__()\n",
    "        self.dtype={'float64':torch.float64,'float32':torch.float32}[options.get('dtype','float64')]\n",
    "        self.device=options.get('device','cpu')\n",
    "        self.max_dim=options.get('max_dim',16)\n",
    "        self.nLayers=options.get('nLayers',20)\n",
    "        self.use_checkpoint=options.get('use_checkpoint',True)\n",
    "        self.observable_checkerboard=False\n",
    "        \n",
    "        self.params=torch.nn.ParameterDict({\n",
    "            k:torch.nn.Parameter(torch.tensor(v,dtype=self.dtype,device=self.device)) for k,v in params.items()\n",
    "        })\n",
    "        self.persistent={}\n",
    "        self.persistent['logZ']=0\n",
    "        \n",
    "    def __str__(self):\n",
    "        rtval=\"\"\n",
    "        for k,v in self.params.items():\n",
    "            rtval+=k+':'+v+'\\n'\n",
    "        rtval+='dtype:'+self.dtype+'\\n'\n",
    "        rtval+='device:'+self.device+'\\n'\n",
    "        rtval+='max_dim:'+self.max_dim+'\\n'\n",
    "        rtval+='nLayers:'+self.nLayers+'\\n'\n",
    "        rtval+='nSite:'+2**self.nLayers+'\\n'\n",
    "        \n",
    "    def set_params(self,params):\n",
    "        self.params=torch.nn.ParameterDict({\n",
    "            k:torch.nn.Parameter(torch.tensor(v,dtype=self.dtype,device=self.device)) for k,v in params.items()\n",
    "        })\n",
    "        \n",
    "    def toT(self,t):\n",
    "        return torch.tensor(t,dtype=self.dtype,device=self.device)\n",
    "    \n",
    "    def generate_random_Isometry(self,dim1,dim2):\n",
    "        dim=max(dim1,dim2)\n",
    "        A=torch.randn(dim,dim,dtype=self.dtype,device=self.device)\n",
    "        U=torch.matrix_exp(A-A.t())\n",
    "        U=U[:dim1,:dim2]\n",
    "        return U\n",
    "    \n",
    "    def TRG_same_T(self,T,*w):\n",
    "        return self.TRG(T,T,*w)\n",
    "    \n",
    "    def _checkpoint(self,F,*ww):\n",
    "        requires_grad=False\n",
    "        for w in ww:\n",
    "            if w.requires_grad:\n",
    "                requires_grad=True\n",
    "        if self.use_checkpoint and requires_grad:\n",
    "            return torch.utils.checkpoint.checkpoint(F,*ww)\n",
    "        else:\n",
    "            return F(*ww)\n",
    "    \n",
    "    def forward_tensor(self,nLayers):\n",
    "        logTotal=0\n",
    "        T=self.get_T0()\n",
    "        for i in range(nLayers):\n",
    "            w=self.ws[(i*self.w_per_layer):((i+1)*self.w_per_layer)]\n",
    "            T=self._checkpoint(self.TRG_same_T,T,*w)\n",
    "                \n",
    "            norm=torch.linalg.norm(T)\n",
    "            T=T/norm\n",
    "            logTotal=2*logTotal+torch.log(norm)\n",
    "        return T,logTotal\n",
    "    \n",
    "    def forward_tensor_with_observable(self,T_op,nLayers,contract_method=None,start_layer=0):\n",
    "        T=self.get_T0()\n",
    "        for i in range(start_layer):\n",
    "            w=self.ws[(i*self.w_per_layer):((i+1)*self.w_per_layer)]\n",
    "            T=self._checkpoint(self.TRG_same_T,T,*w)\n",
    "            \n",
    "        logTotal=0\n",
    "        print(nLayers)\n",
    "        contracted=torch.zeros((int(nLayers),))\n",
    "        Ts,T_ops=[T],[T_op]\n",
    "        for i in tqdm(range(start_layer,nLayers)):\n",
    "            w=self.ws[(i*self.w_per_layer):((i+1)*self.w_per_layer)]\n",
    "            T1=self._checkpoint(self.TRG_same_T,T,*w)\n",
    "            T2=self._checkpoint(self.TRG,T,T_op,*w)\n",
    "            T3=self._checkpoint(self.TRG,T_op,T,*w)\n",
    "            if self.observable_checkerboard and i<self.spacial_dim:\n",
    "                T3=-T3\n",
    "\n",
    "            T,T_op=T1,(T2+T3)/2\n",
    "            norm=torch.linalg.norm(T)\n",
    "            T,T_op=T/norm,T_op/norm\n",
    "            logTotal=2*logTotal+torch.log(norm)\n",
    "            \n",
    "            if contract_method is not None:\n",
    "                Z=contract(T,contract_method)\n",
    "                Z_op=contract(T_op,contract_method)\n",
    "                contracted[i]=Z_op/Z\n",
    "\n",
    "            Ts.append(T);T_ops.append(T_op)\n",
    "\n",
    "        model.Ts=Ts;model.T_ops=T_ops\n",
    "            \n",
    "        return T,T_op,logTotal,contracted\n",
    "    \n",
    "    \n",
    "    #def dlogZ(self,param):\n",
    "    #    self.requires_grad_(False)\n",
    "    #    self.params[param].requires_grad_(True)\n",
    "    #    self.zero_grad()\n",
    "    #    logZ=self.forward(self.nLayers+self.nLayers_HOSVD)\n",
    "    #    logZ.backward()\n",
    "    #    result=_toN(self.params[param].grad)\n",
    "    #    self.params[param].requires_grad_(False)\n",
    "    #    return result\n",
    "    \n",
    "    def update_single_layer(self,layer):\n",
    "        self.requires_grad_(False)\n",
    "        \n",
    "        for i in range(layer*self.w_per_layer,(layer+1)*self.w_per_layer):\n",
    "            self.ws[i].requires_grad_(True)\n",
    "        self.zero_grad()\n",
    "        \n",
    "        logZ=self.forward(self.nLayers)\n",
    "        logZ.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(layer*self.w_per_layer,(layer+1)*self.w_per_layer):\n",
    "                E=self.ws[i].grad\n",
    "                dim1,dim2=E.shape[0],E.shape[2]\n",
    "                E=E.reshape(dim1*dim1,dim2)\n",
    "                U,S,Vh=torch.linalg.svd(E,full_matrices=False)\n",
    "                UVh=U@Vh\n",
    "                #UVh=svd2UVh(E)\n",
    "                del U,S,Vh,E\n",
    "                \n",
    "                #calculate diff\n",
    "                UVh_old=self.ws[i].reshape(dim1*dim1,dim2)\n",
    "                self.ws_diff[i]=_toN(torch.norm(UVh_old.t()@UVh@UVh.t()@UVh_old-torch.eye(dim2,device=UVh.device)))\n",
    "                del UVh_old\n",
    "                    \n",
    "                self.ws[i].data=UVh.reshape(dim1,dim1,dim2)\n",
    "                del UVh\n",
    "                torch.cuda.empty_cache()\n",
    "        return _toN(logZ)\n",
    "        \n",
    "    def optimize(self,nIter):\n",
    "        self.ws_diff=np.zeros(len(self.ws))\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        if nIter>1:\n",
    "            pbar2=tqdm(range(nIter), leave=False)\n",
    "            pbar2.set_postfix({k:_toN(v) for k,v in self.params.items()})\n",
    "        else:\n",
    "            pbar2=range(nIter)\n",
    "        for i in pbar2:\n",
    "            pbar=tqdm([*range(self.nLayers-1,-1,-1)]+[*range(self.nLayers)], leave=False)\n",
    "            for j in pbar:\n",
    "                ws_shape=self.ws[j*self.w_per_layer].shape\n",
    "                if ws_shape[0]**2>ws_shape[2]:\n",
    "                    self.logZ=self.update_single_layer(j)\n",
    "                #else:\n",
    "                #    print(f'Skip layer {j} shape={ws_shape}')\n",
    "        #lock all grads\n",
    "        for param in self.params.values(): \n",
    "            param.requires_grad_(False)\n",
    "        for i in range(self.nLayers): #slightly faster\n",
    "            self.ws[i].requires_grad_(False)\n",
    "        \n",
    "        self.logZ_diff=np.abs(self.persistent['logZ']-self.logZ)\n",
    "        self.persistent['logZ']=self.logZ\n",
    "        \n",
    "        # normalized by layer weight, number of elements in tensor\n",
    "        # NOT USED but multiply by number of elements in last tensor to better match the effects in output\n",
    "        self.ws_diff_normalized=np.zeros(len(self.ws))\n",
    "        for i in range(self.nLayers):\n",
    "            for j in range(self.w_per_layer):\n",
    "                ij=i*self.w_per_layer+j\n",
    "                self.ws_diff_normalized[ij]=self.ws_diff[ij]/2**i#/torch.numel(self.ws[ij])*torch.numel(self.ws[-1])\n",
    "        # ignore the last layers we take trace directly\n",
    "        # use 10-norm so layers of large error has beter contribution       \n",
    "        self.ws_diff_total=np.average(self.ws_diff_normalized[:-self.w_per_layer*self.spacial_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOTRG(SRG):\n",
    "    def __init__(self,params,options):\n",
    "        super(HOTRG,self).__init__(params,options)\n",
    "        self.nLayers_HOSVD=options.get('nLayers_HOSVD',0)\n",
    "        self.persistent['magnetization']=0\n",
    "        self.persistent['energy']=0\n",
    "    \n",
    "    def create_isometries(self,start_dim,spacial_dim):\n",
    "        ws=[]\n",
    "        bond_dim=[start_dim]*spacial_dim\n",
    "        for i in range(self.nLayers+self.nLayers_HOSVD):\n",
    "            for j in range(1,spacial_dim):\n",
    "                old_dim=bond_dim[j]\n",
    "                new_dim=min(old_dim**2,self.max_dim)\n",
    "                U=self.generate_random_Isometry(old_dim**2,new_dim).view(old_dim,old_dim,new_dim)\n",
    "                ws.append(U.detach())\n",
    "                bond_dim[j]=new_dim\n",
    "            bond_dim=bond_dim[1:]+[bond_dim[0]]\n",
    "        self.ws=torch.nn.ParameterList([\n",
    "            torch.nn.Parameter(v) for v in ws\n",
    "        ])\n",
    "        self.w_per_layer=spacial_dim-1\n",
    "        self.spacial_dim=spacial_dim\n",
    "        self.TRG={2:self.HOTRG2D,3:self.HOTRG3D}[self.spacial_dim]\n",
    "        self.HOSVD={2:self.HOSVD2D,3:self.HOSVD3D}[self.spacial_dim]\n",
    "        self.ws_diff_normalized=np.zeros(len(self.ws))\n",
    "        self.ws_diff=np.zeros(len(self.ws))\n",
    "        self.ws_diff_total=0\n",
    "        \n",
    "        \n",
    "    def HOTRG2D(self,T1,T2,w):\n",
    "        return contract('ijkl,jmno,kna,lob->abim',T1,T2,w,w)#contract and rotate\n",
    "    \n",
    "    def HOTRG3D(self,T1,T2,w1,w2):\n",
    "        return contract('ijklmn,jopqrs,kpa,lqb,mrc,nsd->abcdio',T1,T2,w1,w1,w2,w2)#contract and rotate\n",
    "    \n",
    "    def HOSVD2D(self,T1,T2):\n",
    "        MM1=contract('ijkl,jmno,ipql,pmro->knqr',T1,T2,T1.conj(),T2.conj()).reshape(T1.shape[2]*T2.shape[2],-1)\n",
    "        S1,U1=torch.linalg.eigh(MM1) #S1 ascending U S Uh=MM\n",
    "        eps1=torch.sum(torch.abs(S1[-self.max_dim:])) # might be slightly minus due to numerical error\n",
    "        \n",
    "        MM2=contract('ijkl,jmno,ipql,pmro->knqr',T1.transpose(2,3),T2.transpose(2,3),T1.conj().transpose(2,3),T2.conj().transpose(2,3)).reshape(T1.shape[3]*T2.shape[3],-1)\n",
    "        S2,U2=torch.linalg.eigh(MM2)\n",
    "        eps2=torch.sum(torch.abs(S2[-self.max_dim:]))\n",
    "\n",
    "        S,U=(S1,U1) if eps1<eps2 else (S2,U2)\n",
    "        S,U=S1,U1\n",
    "        w=U[:,-self.max_dim:].reshape(T1.shape[2],T2.shape[2],-1)\n",
    "        return contract('ijkl,jmno,kna,lob->abim',T1,T2,w,w),[w]\n",
    "    \n",
    "    def HOSVD3D(self,T1,T2):\n",
    "        #print(T1.shape)\n",
    "        MM1=contract('ijklmn,jopqrs,itulmn,tovqrs->kpuv',T1,T2,T1.conj(),T2.conj()).reshape(T1.shape[2]*T2.shape[2],-1)\n",
    "        S1,U1=torch.linalg.eigh(MM1) #S1 ascending U S Uh=MM\n",
    "        eps1=torch.sum(torch.abs(S1[-self.max_dim:])) # might be slightly minus due to numerical error\n",
    "        \n",
    "        MM2=contract('ijklmn,jopqrs,itulmn,tovqrs->kpuv',T1.transpose(2,3),T2.transpose(2,3),T1.conj().transpose(2,3),T2.conj().transpose(2,3)).reshape(T1.shape[3]*T2.shape[3],-1)\n",
    "        S2,U2=torch.linalg.eigh(MM2)\n",
    "        eps2=torch.sum(torch.abs(S2[-self.max_dim:]))\n",
    "\n",
    "        S,U=(S1,U1) if eps1<eps2 else (S2,U2)\n",
    "        w1=U[:,-self.max_dim:].reshape(T1.shape[2],T2.shape[2],-1)\n",
    "        \n",
    "        MM1=contract('ijklmn,jopqrs,itklun,topqvs->mruv',T1,T2,T1.conj(),T2.conj()).reshape(T1.shape[4]*T2.shape[4],-1)\n",
    "        S1,U1=torch.linalg.eigh(MM1) #S1 ascending U S Uh=MM\n",
    "        eps1=torch.sum(torch.abs(S1[-self.max_dim:])) # might be slightly minus due to numerical error\n",
    "        \n",
    "        MM2=contract('ijklmn,jopqrs,itklun,topqvs->mruv',T1.transpose(4,5),T2.transpose(4,5),T1.conj().transpose(4,5),T2.conj().transpose(4,5)).reshape(T1.shape[5]*T2.shape[5],-1)\n",
    "        S2,U2=torch.linalg.eigh(MM2)\n",
    "        eps2=torch.sum(torch.abs(S2[-self.max_dim:]))\n",
    "\n",
    "        S,U=(S1,U1) if eps1<eps2 else (S2,U2)\n",
    "        w2=U[:,-self.max_dim:].reshape(T1.shape[4],T2.shape[4],-1)\n",
    "        #print(w1.shape,w2.shape)\n",
    "        \n",
    "        return contract('ijklmn,jopqrs,kpa,lqb,mrc,nsd->abcdio',T1,T2,w1,w1,w2,w2),[w1,w2]\n",
    "    \n",
    "    \n",
    "    def generate_isometries_HOSVD(self):\n",
    "        with torch.no_grad():\n",
    "            logTotal=0\n",
    "            T=self.get_T0()\n",
    "            Ts=[T]\n",
    "            for i in tqdm(range(self.nLayers+self.nLayers_HOSVD), leave=False):\n",
    "                T,ww=self.HOSVD(T,T)\n",
    "                for j in range(self.w_per_layer):\n",
    "                    self.ws[i*self.w_per_layer+j].data=ww[j]\n",
    "                norm=torch.linalg.norm(T)\n",
    "                T=T/norm\n",
    "                logTotal=2*logTotal+torch.log(norm)\n",
    "                Ts.append(T)\n",
    "\n",
    "            contract_all=[i for i in range(len(T.shape)//2) for j in range(2)]\n",
    "            Z=contract(T,contract_all)\n",
    "            self.persistent['logZ']=self.logZ=_toN((torch.log(Z)+logTotal)/2**self.nLayers)\n",
    "            self.Ts=Ts\n",
    "    \n",
    "    def forward(self,nLayers):\n",
    "        T,logTotal=self.forward_tensor(nLayers)\n",
    "        contract_all=[i for i in range(len(T.shape)//2) for j in range(2)]\n",
    "        Z=contract(T,contract_all)\n",
    "        return (torch.log(Z)+logTotal)/2**nLayers\n",
    "    \n",
    "    def forward_with_observable(self,T_op,nLayers,start_layer=0):\n",
    "        contract_method=[i for i in range(len(T_op.shape)//2) for j in range(2)]\n",
    "        _,_,_,contracted=self.forward_tensor_with_observable(T_op,nLayers,contract_method=contract_method,start_layer=start_layer)\n",
    "        return contracted,contracted[-1]\n",
    "    \n",
    "    #def forward_and_HOTRG(self):\n",
    "    #    T,logTotal=self.forward_tensor()\n",
    "    #    for i in range(self.nLayers_HOSVD):\n",
    "    #        #T,_=self.HOSVD(T,T)\n",
    "    #        ws=\n",
    "    #        T=self.TRG(T,T,*ws)\n",
    "    #        norm=torch.linalg.norm(T)\n",
    "    #        T=T/norm\n",
    "    #        logTotal=2*logTotal+torch.log(norm)\n",
    "    #    contract_all=[i for i in range(len(T.shape)//2) for j in range(2)]\n",
    "    #    Z=contract(T,contract_all)\n",
    "    #    return (torch.log(Z)+logTotal)/2**(self.nLayers+self.nLayers_HOSVD)\n",
    "    #\n",
    "    #def forward_with_observable_and_HOTRG(self,T_op,start_layer=0):\n",
    "    #    contract_method=[i for i in range(len(T_op.shape)//2) for j in range(2)]\n",
    "    #    T,T_op,logTotal,contracted=self.forward_tensor_with_observable(T_op,contract_method=contract_method,start_layer=start_layer)\n",
    "    #    \n",
    "    #    for i in range(self.nLayers_HOSVD):\n",
    "    #        T1,ws=self.HOSVD(T,T)\n",
    "    #        T2=self.TRG(T,T_op,*ws)\n",
    "    #        T3=self.TRG(T_op,T,*ws)\n",
    "    #        \n",
    "    #        T,T_op=T1,(T2+T3)/2\n",
    "    #        norm=torch.linalg.norm(T)\n",
    "    #        T,T_op=T/norm,T_op/norm\n",
    "    #        logTotal=2*logTotal+torch.log(norm)\n",
    "    #        \n",
    "    #    Z=contract(T,contract_method)\n",
    "    #    Z_op=contract(T_op,contract_method)\n",
    "    #    \n",
    "    #    return contracted,Z_op/Z\n",
    "    \n",
    "    def calc_logZ(self):\n",
    "        with torch.no_grad():\n",
    "            self.logZ=_toN(self.forward(self.nLayers+self.nLayers_HOSVD))\n",
    "            self.logZ_diff=np.abs(self.persistent['logZ']-self.logZ)\n",
    "            self.persistent['logZ']=self.logZ\n",
    "    \n",
    "    def calc_magnetization(self):\n",
    "        with torch.no_grad():\n",
    "            print(self.nLayers,self.nLayers_HOSVD)\n",
    "            a,b=self.forward_with_observable(self.get_SZT0(),self.nLayers+self.nLayers_HOSVD)\n",
    "            self.magnetization_per_layer=_toN(torch.abs(a))\n",
    "            self.magnetization=_toN(torch.abs(b))\n",
    "            self.magnetization_diff=np.abs(self.persistent['magnetization']-self.magnetization)\n",
    "            self.persistent['magnetization']=self.magnetization\n",
    "            \n",
    "    def calc_energy(self):\n",
    "        with torch.no_grad():\n",
    "            a,b=self.forward_with_observable(self.get_ET1(),self.nLayers+self.nLayers_HOSVD,start_layer=1)\n",
    "            self.energy_per_layer=_toN(a)\n",
    "            self.energy=_toN(b)\n",
    "            self.energy_diff=np.abs(self.persistent['energy']-self.energy)\n",
    "            self.persistent['energy']=self.energy\n",
    "            \n",
    "            \n",
    "def NewRow(model,params,options):\n",
    "    return {**params,**options,\n",
    "                'logZ':model.logZ,\n",
    "                'logZ_diff':model.logZ_diff,\n",
    "                'magnetization':model.magnetization,\n",
    "                'magnetization_diff':model.magnetization_diff,\n",
    "                'magnetization_per_layer':model.magnetization_per_layer.copy(),\n",
    "                'energy':model.energy,\n",
    "                'energy_diff':model.energy_diff,\n",
    "                'energy_per_layer':model.energy_per_layer.copy(),\n",
    "                'ws_diff_total':model.ws_diff_total,\n",
    "                'ws_diff':model.ws_diff.copy(),\n",
    "                'ws_diff_normalized':model.ws_diff_normalized.copy(),\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "def get_CG_no_normalization(n):\n",
    "    if n==0:\n",
    "        return np.eye(1)\n",
    "    CG=np.zeros((n+1,)+(2,)*n)\n",
    "    for i in range(2**n):\n",
    "        indices=tuple(map(int,bin(i)[2:].zfill(n)))\n",
    "        m=np.sum(indices)\n",
    "        CG[(m,)+indices]=1\n",
    "    return CG\n",
    "def get_CG(n):\n",
    "    if n==0:\n",
    "        return np.eye(1)\n",
    "    CG=np.zeros((n+1,)+(2,)*n)\n",
    "    for i in range(2**n):\n",
    "        indices=tuple(map(int,bin(i)[2:].zfill(n)))\n",
    "        m=np.sum(indices)\n",
    "        CG[(m,)+indices]=1/np.sqrt(comb(n,m))\n",
    "    return CG\n",
    "def get_Singlet():\n",
    "    return np.array([[0,1.],[-1.,0]])\n",
    "\n",
    "\n",
    "class AKLT2D(HOTRG):\n",
    "    default_params={'a1':np.sqrt(6)/2,'a2':np.sqrt(6)}\n",
    "    def __init__(self,params,options):\n",
    "        super(AKLT2D,self).__init__(params,options)\n",
    "        self.create_isometries(start_dim=4,spacial_dim=2)\n",
    "        #self.TRG=self.HOTRG2D\n",
    "        self.observable_checkerboard=True\n",
    "        #self.persistent['magnetization']=0\n",
    "        \n",
    "    def get_T0(self):\n",
    "        projector=self.toT(get_CG_no_normalization(4))\n",
    "        singlet=self.toT([[0,-1],[1,0]])\n",
    "        ac0,ac1,ac2=self.toT(1),self.params['a1'],self.params['a2']\n",
    "        deform=torch.stack([ac2,ac1,ac0,ac1,ac2])\n",
    "        #deform=torch.stack([ac2*(1-2e-6),ac1*(1-1e-6),ac0,ac1*(1+1e-6),ac2*(1+2e-6)])\n",
    "        node=contract('aijkl,im,kn,a->amjnl',projector,singlet,singlet,deform)\n",
    "        return contract('aijkl,amnop->imjnkolp',node,node).reshape(4,4,4,4)#UDLR\n",
    "\n",
    "    def get_SZT0(self):\n",
    "        projector=self.toT(get_CG_no_normalization(4))\n",
    "        singlet=self.toT([[0,-1],[1,0]])\n",
    "        ac0,ac1,ac2=self.toT(1),self.params['a1'],self.params['a2']\n",
    "        deform=torch.stack([ac2,ac1,ac0,ac1,ac2])\n",
    "        #deform=torch.stack([ac2*(1-2e-6),ac1*(1-1e-6),ac0,ac1*(1+1e-6),ac2*(1+2e-6)])\n",
    "        node=contract('aijkl,im,kn,a->amjnl',projector,singlet,singlet,deform)\n",
    "        op=self.toT([2,1,0,-1,-2])\n",
    "        return contract('aijkl,amnop,a->imjnkolp',node,node,op).reshape(4,4,4,4)#UDLR\n",
    "    \n",
    "\n",
    "class AKLT3D(HOTRG):\n",
    "    default_params={'a1':np.sqrt(20/15),'a2':np.sqrt(20/6),'a3':np.sqrt(20/1)}\n",
    "    def __init__(self,params,options):\n",
    "        super(AKLT3D,self).__init__(params,options)\n",
    "        self.create_isometries(start_dim=4,spacial_dim=3)\n",
    "        #self.TRG=self.HOTRG3D\n",
    "        self.observable_checkerboard=True\n",
    "        #self.persistent['magnetization']=0\n",
    "        \n",
    "    def get_T0(self):\n",
    "        projector=self.toT(get_CG_no_normalization(6))\n",
    "        singlet=self.toT([[0,-1],[1,0]])\n",
    "        ac0,ac1,ac2,ac3=self.toT(1),self.params['a1'],self.params['a2'],self.params['a3']\n",
    "        deform=torch.stack([ac3,ac2,ac1,ac0,ac1,ac2,ac3])\n",
    "        #deform=torch.stack([ac3*(1-3e-6),ac2*(1-2e-6),ac1*(1-1e-6),ac0,ac1*(1+1e-6),ac2*(1+2e-6),ac3*(1+3e-6)])\n",
    "        node=contract('aijklmn,io,kp,mq,a->aojplqn',projector,singlet,singlet,singlet,deform)\n",
    "        return contract('aijklmn,aopqrst->iojpkqlrmsnt',node,node).reshape(4,4,4,4,4,4)#UDLRFB\n",
    "\n",
    "    def get_SZT0(self):\n",
    "        projector=self.toT(get_CG_no_normalization(6))\n",
    "        singlet=self.toT([[0,-1],[1,0]])\n",
    "        ac0,ac1,ac2,ac3=self.toT(1),self.params['a1'],self.params['a2'],self.params['a3']\n",
    "        deform=torch.stack([ac3,ac2,ac1,ac0,ac1,ac2,ac3])\n",
    "        #deform=torch.stack([ac3*(1-3e-6),ac2*(1-2e-6),ac1*(1-1e-6),ac0,ac1*(1+1e-6),ac2*(1+2e-6),ac3*(1+3e-6)])\n",
    "        node=contract('aijklmn,io,kp,mq,a->aojplqn',projector,singlet,singlet,singlet,deform)\n",
    "        op=self.toT([3,2,1,0,-1,-2,-3])\n",
    "        return contract('aijklmn,aopqrst,a->iojpkqlrmsnt',node,node,op).reshape(4,4,4,4,4,4)#UDLRFB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b061bced4de545bc87b093fb024f2db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 0\n",
      "15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15161155640b419bbf09cbd2b1473360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options={\n",
    "    'dtype':'float64',\n",
    "    'device':'cuda:0',\n",
    "    'max_dim':10, # 10 discussed with wei\n",
    "    'nLayers':15, # 30\n",
    "    'use_checkpoint':True\n",
    "}\n",
    "params=AKLT3D.default_params.copy()\n",
    "params['a1']+=.008\n",
    "model=AKLT3D(params,options)\n",
    "model.generate_isometries_HOSVD()\n",
    "\n",
    "# model.calc_logZ()\n",
    "model.calc_magnetization()\n",
    "# print(model.magnetization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.set_default_tensor_type(torch.cuda.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(3.5552844e-06, dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.magnetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0, 0.0),\n",
       " (-2.7897324047749574, 0.04779658942265784, -0.13333969435011256),\n",
       " (-1.821997652876098, 1.0153369211760674, -1.8499414872612385),\n",
       " (1.4392149863065409, 1.192677601453903, 1.716519477844597),\n",
       " (1.0142565934901129, 1.030020219631409, 1.0447047991892908),\n",
       " (0.8214542841935257, 0.8949524798322831, 0.7351625487078489),\n",
       " (0.72428624019466, 1.030275604994425, 0.7462144443056908),\n",
       " (0.8787220902480029, 0.9880340700172763, 0.8682073632418226),\n",
       " (1.0420737876185537, 0.9505671077898153, 0.990561066400147),\n",
       " (1.0852982191023912, 0.9742598610125194, 1.0573624920998306),\n",
       " (1.0945176855210959, 0.9899900614161253, 1.083561630710065),\n",
       " (1.0951954357030456, 0.993951124144083, 1.088570734474511),\n",
       " (1.0952695158500627, 0.9955685004135967, 1.090415829443573),\n",
       " (1.0952769848202044, 0.9967958559812493, 1.0917675596204175),\n",
       " (1.0952768607987813, 0.9973498644867291, 1.0923742286931146),\n",
       " (1.0952768238304655, 0.9974733446743865, 1.0925094368105133)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from HOTRG import trace_two_tensors\n",
    "tt=[(trace_two_tensors(T).item(),trace_two_tensors(T_op).item()) for T,T_op in zip(model.Ts,model.T_ops)]\n",
    "[((b/a if abs(a)>0 else 0),a,b) for a,b in tt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HOTRG import HOTRGLayer\n",
    "T0=model.get_T0()\n",
    "T0_op=model.get_SZT0()\n",
    "layers=[HOTRGLayer(tensor_shape=model.Ts[i].shape,ww=[model.ws[2*i],model.ws[2*i+1]]) for i in range(model.nLayers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3bb2b95f644a52bc6f8582451a0117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc654fa0ca9492ca8e04cc9f36643f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 764.00 MiB (GPU 0; 11.17 GiB total capacity; 9.37 GiB already allocated; 524.25 MiB free; 10.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mHOTRG\u001b[39;00m \u001b[39mimport\u001b[39;00m forward_observable_tensor\n\u001b[0;32m----> 2\u001b[0m Ts,T_ops,logTotals\u001b[39m=\u001b[39mforward_observable_tensor(T0,T0_op,layers)\n",
      "File \u001b[0;32m~/jupyter/AKLT3D/HOTRG.py:123\u001b[0m, in \u001b[0;36mforward_observable_tensor\u001b[0;34m(T0, T0_op, layers, start_layer, checkerboard, use_checkpoint, return_layers, cached_Ts)\u001b[0m\n\u001b[1;32m    121\u001b[0m     T1\u001b[39m=\u001b[39mforward_layer(T,T,layer\u001b[39m=\u001b[39mlayer,use_checkpoint\u001b[39m=\u001b[39muse_checkpoint)\n\u001b[1;32m    122\u001b[0m \u001b[39m#with BypassGilt(False,True):\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m T2\u001b[39m=\u001b[39mforward_layer(T,T_op,layer\u001b[39m=\u001b[39;49mlayer,use_checkpoint\u001b[39m=\u001b[39;49muse_checkpoint)\n\u001b[1;32m    124\u001b[0m \u001b[39m#with BypassGilt(True,False):\u001b[39;00m\n\u001b[1;32m    125\u001b[0m T3\u001b[39m=\u001b[39mforward_layer(T_op,T,layer\u001b[39m=\u001b[39mlayer,use_checkpoint\u001b[39m=\u001b[39muse_checkpoint)\n",
      "File \u001b[0;32m~/jupyter/AKLT3D/HOTRG.py:77\u001b[0m, in \u001b[0;36mforward_layer\u001b[0;34m(Ta, Tb, layer, use_checkpoint)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_layer\u001b[39m(Ta,Tb,layer:HOTRGLayer,use_checkpoint\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m-\u001b[39m\u001b[39m>\u001b[39mtorch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     76\u001b[0m     \u001b[39m#_forward_layer={4:_forward_layer_2D,6:_forward_layer_3D}[len(Ta.shape)]\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mreturn\u001b[39;00m _checkpoint(_forward_layer,[Ta,Tb],{\u001b[39m'\u001b[39;49m\u001b[39mlayer\u001b[39;49m\u001b[39m'\u001b[39;49m:layer},use_checkpoint\u001b[39m=\u001b[39;49muse_checkpoint)\n",
      "File \u001b[0;32m~/jupyter/AKLT3D/HOTRG.py:73\u001b[0m, in \u001b[0;36m_checkpoint\u001b[0;34m(function, args, args1, use_checkpoint)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(wrapper,\u001b[39m*\u001b[39margs)\n\u001b[1;32m     72\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs,\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49margs1)\n",
      "File \u001b[0;32m~/jupyter/AKLT3D/HOTRG.py:64\u001b[0m, in \u001b[0;36m_forward_layer\u001b[0;34m(Ta, Tb, layer)\u001b[0m\n\u001b[1;32m     60\u001b[0m insertion\u001b[39m=\u001b[39mlayer\u001b[39m.\u001b[39mget_insertion()\n\u001b[1;32m     61\u001b[0m eq\u001b[39m=\u001b[39m{\u001b[39m4\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mijkl,Jmno,jJ,xi,ym,akn,blo->abxy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     62\u001b[0m     \u001b[39m6\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mijklmn,Jopqrs,jJ,xi,yo,akp,blq,cmr,dns->abcdxy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     63\u001b[0m     }[\u001b[39mlen\u001b[39m(layer\u001b[39m.\u001b[39mtensor_shape)]\n\u001b[0;32m---> 64\u001b[0m T\u001b[39m=\u001b[39mcontract(eq,Ta,Tb,insertion,\u001b[39m*\u001b[39;49misometries)\n\u001b[1;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m T\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/opt_einsum/contract.py:507\u001b[0m, in \u001b[0;36mcontract\u001b[0;34m(*operands, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m gen_expression:\n\u001b[1;32m    505\u001b[0m     \u001b[39mreturn\u001b[39;00m ContractExpression(full_str, contraction_list, constants_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39meinsum_kwargs)\n\u001b[0;32m--> 507\u001b[0m \u001b[39mreturn\u001b[39;00m _core_contract(operands, contraction_list, backend\u001b[39m=\u001b[39;49mbackend, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49meinsum_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/opt_einsum/contract.py:573\u001b[0m, in \u001b[0;36m_core_contract\u001b[0;34m(operands, contraction_list, backend, evaluate_constants, **einsum_kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m     right_pos\u001b[39m.\u001b[39mappend(input_right\u001b[39m.\u001b[39mfind(s))\n\u001b[1;32m    572\u001b[0m \u001b[39m# Contract!\u001b[39;00m\n\u001b[0;32m--> 573\u001b[0m new_view \u001b[39m=\u001b[39m _tensordot(\u001b[39m*\u001b[39;49mtmp_operands, axes\u001b[39m=\u001b[39;49m(\u001b[39mtuple\u001b[39;49m(left_pos), \u001b[39mtuple\u001b[39;49m(right_pos)), backend\u001b[39m=\u001b[39;49mbackend)\n\u001b[1;32m    575\u001b[0m \u001b[39m# Build a new view if needed\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[39mif\u001b[39;00m (tensor_result \u001b[39m!=\u001b[39m results_index) \u001b[39mor\u001b[39;00m handle_out:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/opt_einsum/sharing.py:131\u001b[0m, in \u001b[0;36mtensordot_cache_wrap.<locals>.cached_tensordot\u001b[0;34m(x, y, axes, backend)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(tensordot)\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcached_tensordot\u001b[39m(x, y, axes\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, backend\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m currently_sharing():\n\u001b[0;32m--> 131\u001b[0m         \u001b[39mreturn\u001b[39;00m tensordot(x, y, axes, backend\u001b[39m=\u001b[39;49mbackend)\n\u001b[1;32m    133\u001b[0m     \u001b[39m# hash based on the (axes_x,axes_y) form of axes\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     _save_tensors(x, y)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/opt_einsum/contract.py:374\u001b[0m, in \u001b[0;36m_tensordot\u001b[0;34m(x, y, axes, backend)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[39m\"\"\"Base tensordot.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    373\u001b[0m fn \u001b[39m=\u001b[39m backends\u001b[39m.\u001b[39mget_func(\u001b[39m'\u001b[39m\u001b[39mtensordot\u001b[39m\u001b[39m'\u001b[39m, backend)\n\u001b[0;32m--> 374\u001b[0m \u001b[39mreturn\u001b[39;00m fn(x, y, axes\u001b[39m=\u001b[39;49maxes)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/opt_einsum/backends/torch.py:54\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(x, y, axes)\u001b[0m\n\u001b[1;32m     51\u001b[0m torch, _ \u001b[39m=\u001b[39m _get_torch_and_device()\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m _TORCH_HAS_TENSORDOT:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensordot(x, y, dims\u001b[39m=\u001b[39;49maxes)\n\u001b[1;32m     56\u001b[0m xnd \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mndimension()\n\u001b[1;32m     57\u001b[0m ynd \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mndimension()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/torch/functional.py:1092\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(a, b, dims, out)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     dims_b \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(dims))\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1092\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mtensordot(a, b, dims_a, dims_b)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1094\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mtensordot(a, b, dims_a, dims_b, out\u001b[39m=\u001b[39mout)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 764.00 MiB (GPU 0; 11.17 GiB total capacity; 9.37 GiB already allocated; 524.25 MiB free; 10.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from HOTRG import forward_observable_tensor\n",
    "Ts,T_ops,logTotals=forward_observable_tensor(T0,T0_op,layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
